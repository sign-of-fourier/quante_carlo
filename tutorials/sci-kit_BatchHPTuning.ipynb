{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6369e92b-8114-4736-8782-2448501e0b5b",
   "metadata": {},
   "source": [
    "# How to do batch hyperparmaeter tuning with sci-kit using quante_carlo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d0dafc-f071-4050-9398-7c18ea8a0fbe",
   "metadata": {},
   "source": [
    "This is a demonstration that trains sci-kit models on toy datasets and makes reuqests to bayesian optimization as a service for hyperparameter tuning.<br>\n",
    "Note to person giving demo:<br>\n",
    "    <code>gunicorn -w 18 'flask_worker:app'</code><br>\n",
    "(if you hvae 18 processors to run the bo)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "897f9ab8-853b-4258-9dbf-b0684c2a9fc1",
   "metadata": {},
   "source": [
    "<code>pip install --extra-index-url=https://pypi.nvidia.com cudf-cu12==24.10.* cuml-cu12==24.10.* </cide>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d7ab8a-ae8d-48fa-bf76-35fadefe5afb",
   "metadata": {},
   "source": [
    "For this demo, there is no actual parallel training. The purpose of this demo is so that you can see how the parallel HPT works even if you don't have the ability to do training in parallel."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec63643-7bfb-4dbe-9d9f-5c899754017e",
   "metadata": {},
   "source": [
    "## Load Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37eeda8c-b6f8-4677-8b2f-70c235e4c186",
   "metadata": {},
   "source": [
    "### Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f71e12e8-423a-4723-a98e-d75352e44b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests \n",
    "import json\n",
    "import random\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.datasets import load_diabetes, load_digits, load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "diabetes = load_diabetes()\n",
    "digits = load_diabetes()\n",
    "breast_cancer = load_breast_cancer()\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import r2_score\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import time\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.svm import SVR\n",
    "from xgboost import XGBClassifier, XGBRegressor\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "import cupy as cp\n",
    "from cuml.ensemble import RandomForestRegressor as curfr\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "import os\n",
    "from sklearn.tree import DecisionTreeRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "218660bf-6f74-4e48-a8b3-990c93b0edf5",
   "metadata": {},
   "source": [
    "### User defined modules"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3894d031-b66b-4a81-b94b-35f8c9530350",
   "metadata": {},
   "source": [
    "#### Random forest implementation using NVIDIA's rapids library\n",
    "Runs RF in batch mode due to very large data frame size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05fb8931-3e85-4f29-a735-5596457c718d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class batch_rf:\n",
    "    def __init__(self, max_depth, min_samples_leaf, min_samples_split, impurity_decrease):\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_leaf = min_samples_leaf\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.impurity_decrease = impurity_decrease\n",
    "        self.all_predictions = []\n",
    "        self.truth = []\n",
    "\n",
    "    def get_file(self, path, f):\n",
    "        if f == 'xaa':\n",
    "            nvidia_df = pd.read_csv(path + '/' + f, nrows=100)\n",
    "        else:\n",
    "            headers = pd.read_csv(path + '/xaa', nrows=1)\n",
    "            nvidia_df = pd.read_csv(path + '/' + f, nrows=100, header=None)\n",
    "            nvidia_df.columns = headers.columns\n",
    "            \n",
    "        float_cols = [c for c in nvidia_df if nvidia_df[c].dtype == \"float64\"]\n",
    "        float32_cols = {c: np.float32 for c in float_cols}\n",
    "        if f == 'xaa':\n",
    "            nvidia_df = pd.read_csv(path + '/' + f, dtype=float32_cols)\n",
    "        else:\n",
    "            nvidia_df = pd.read_csv(path + '/' + f, dtype=float32_cols, header=None)\n",
    "            nvidia_df.columns = headers.columns\n",
    "        return nvidia_df\n",
    "        \n",
    "    def prep(self, df, enc=None, kmodel=None):\n",
    "            \n",
    "        continuous_predictors = df.columns[4:]\n",
    "        categorical = ['trickortreat', 'kingofhalloween']\n",
    "        if not enc:\n",
    "            enc = OneHotEncoder(handle_unknown='ignore')\n",
    "            enc = enc.fit(df[categorical])\n",
    "            \n",
    "        categorical_df = enc.transform(df[categorical]).astype('float32')\n",
    "    \n",
    "        if not kmodel:\n",
    "            k = KMeans(n_clusters=20)\n",
    "            k.fit(categorical_df)\n",
    "            leaf_ids = k.predict(categorical_df)\n",
    "        else:\n",
    "            k = kmodel\n",
    "            leaf_ids = kmodel.predict(categorical_df)\n",
    "            \n",
    "        df2 = df[continuous_predictors].copy()\n",
    "        df2['dtree_id'] = leaf_ids.astype('float32')\n",
    "        df2 = df2.copy().astype('float32')\n",
    "        \n",
    "        # impute missing\n",
    "        for c in df2.columns:\n",
    "            m = df2[c].mean()\n",
    "            df2[c] = df2[c].fillna(value=m)\n",
    "                \n",
    "        return df2, enc, k\n",
    "        \n",
    "\n",
    "    def fit(self, path):\n",
    "                \n",
    "        files = os.listdir(path)\n",
    "        for f in files[:3]:\n",
    "            if f[0] == 'x':\n",
    "                df = self.get_file(path, f)\n",
    "                df2, enc, kmodel = self.prep(df)\n",
    "                \n",
    "                X_train, X_test, y_train, y_test = train_test_split(df2, df['y'], test_size=0.3, random_state=42)\n",
    "            \n",
    "                rf = curfr(max_depth=self.max_depth, \n",
    "                           min_samples_leaf=self.min_samples_leaf,\n",
    "                           min_samples_split=self.min_samples_split,\n",
    "                           min_impurity_decrease=self.impurity_decrease,\n",
    "                           n_estimators=200, accuracy_metric='rmse')\n",
    "\n",
    "                rf.fit(X_train, y_train)\n",
    "                \n",
    "                predictions = []        \n",
    "                for g in files[:3]:\n",
    "                    if g[0] == 'x':\n",
    "                        if g[0] != f[0]:\n",
    "                            df2 = self.get_file(path, g)\n",
    "                            df3 = self.prep(df2, enc, kmodel)\n",
    "                            X_train2, X_test2, y_train2, y_test2 = train_test_split(df3, df2['y'], test_size=0.3, random_state=42)\n",
    "                            predictions.append(rf.predict(X_test2))\n",
    "                        else:\n",
    "                            predictions.append(rf.predict(X_test))\n",
    "                            self.truth.append(y_test.copy())\n",
    "\n",
    "                self.all_predictions.append(predictions.copy())\n",
    "                \n",
    "        return self\n",
    "        \n",
    "    def score(self):\n",
    "        sq_error = 0\n",
    "        sum_of_squares = 0\n",
    "        m = 0\n",
    "        s = 0\n",
    "        for t in self.truth:\n",
    "            m += np.sum(t)\n",
    "            s += len(t)\n",
    "            \n",
    "        for a in range(len(self.all_predictions)):\n",
    "            sq_error += np.sum([(p-t)**2 for p, t in zip(self.all_predictions[a], self.truth[a])])\n",
    "            sum_of_squares += np.sum([(p-m/s)**2 for p in self.all_predictions[a]])\n",
    "        return 2-sq_error/sum_of_squares\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e31c9e96-6ff3-40ad-8988-cf500a9d09ea",
   "metadata": {},
   "source": [
    "#### Wrapper for multiple different models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab565e5-97d3-41df-8726-16efc030fb99",
   "metadata": {},
   "outputs": [],
   "source": [
    "class bayes_optimization_example:\n",
    "    def __init__(self, gbr_batch_size, n_processors, model):\n",
    "        self.n_procs = n_processors\n",
    "        self.gbr_batch_size = gbr_batch_size # how many points to evaluate when optimizing gaussian process\n",
    "        self.model = model\n",
    "\n",
    "    def metric(self, x, y):\n",
    "#        if self.target_type == 'regression':\n",
    "#            if x < y:\n",
    "#                return True\n",
    "#            else:\n",
    "#                return False\n",
    "#        else:\n",
    "        if x > y:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "            \n",
    "    def model_selection(self, r):\n",
    "        if self.model == 'ElasticNet':\n",
    "            self.target_type = 'regression'\n",
    "            model = ElasticNet(alpha = r[0], l1_ratio=r[1])\n",
    "        elif self.model == 'SVR':\n",
    "            self.target_type = 'regression'\n",
    "            model = make_pipeline(StandardScaler(), SVR(C=r[0], epsilon=r[1]))\n",
    "        elif self.model == 'XGBoostClassifier':\n",
    "            self.target_type = 'classification'\n",
    "            model = XGBClassifier(gamma=r[0], reg_lambda=r[1], colsample_bytree=r[2], \n",
    "                                  max_depth=r[3], min_child_weight=r[4], learning_rate=r[5])\n",
    "        elif self.model == 'XGBoostRegressor':\n",
    "            self.target_type = 'regression'\n",
    "            model = XGBRegressor(gamma=r[0], reg_lambda=r[1], colsample_bytree=r[2], \n",
    "                                 max_depth=r[3], min_child_weight=r[4], learning_rate=r[5])\n",
    "        elif self.model == 'RandomForestRegressor':\n",
    "            self.target_type = 'regression'\n",
    "            model = batch_rf(r[0], r[1], r[2], r[3])\n",
    "#            model = curfr(max_depth=r[0], n_bins=r[2],\n",
    "#            model = curfr(max_depth=r[0], max_features=1.0, n_bins=512,\n",
    "#                          min_samples_leaf=r[3],\n",
    "#                          min_samples_split=r[4],\n",
    "#                          min_impurity_decrease=r[5],\n",
    "#                          n_estimators=200, accuracy_metric='r2')\n",
    "        else:\n",
    "            print('No Model')\n",
    "        return model\n",
    "    \n",
    "    def initialize(self, toy_data, hp_types, hp_ranges):\n",
    "        \n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(toy_data.data, toy_data.target, test_size=0.3, random_state=42)\n",
    "        self.data = toy_data.data\n",
    "        self.target = toy_data.target\n",
    "        self.hp_types = hp_types\n",
    "        self.historical_scores = []\n",
    "        self.historical_points = []\n",
    "        self.hp_ranges = hp_ranges\n",
    "        for a in range(3):\n",
    "            r = [random.uniform(hp_ranges[x][0], hp_ranges[x][1]) \n",
    "                 if hp_types[x] == 'float' else random.randint(hp_ranges[x][0], hp_ranges[x][1]) \n",
    "                 for x in range(len(hp_ranges))]\n",
    "\n",
    "            model = self.model_selection(r)\n",
    "\n",
    "            if self.model != 'RandomForestRegressor':\n",
    "                model.fit(self.X_train, self.y_train)\n",
    "                if self.target_type == 'regression':\n",
    "                    self.historical_scores.append(str(r2_score(self.y_test, model.predict(self.X_test))))\n",
    "                else:\n",
    "                    self.historical_scores.append(str(cross_val_score(model, toy_data.data, toy_data.target).mean()))                \n",
    "            else:\n",
    "                fitted_model = model.fit('split_training_data')\n",
    "                self.historical_scores.append(fitted_model.score())\n",
    "\n",
    "                \n",
    "            self.historical_points.append(','.join([str(x) for x in r]))\n",
    "\n",
    "    def test_points(self, next_points):\n",
    "        \n",
    "        for nxt_pt in [p.split(',') for p in next_points['next_points'].split(';')]:\n",
    "            r = [float(nxt_pt[x]) if self.hp_types[x] == 'float' else int(nxt_pt[x]) for x in range(len(nxt_pt))]\n",
    "            \n",
    "            model = self.model_selection(r)\n",
    "            \n",
    "            if self.model != 'RandomForestRegressor':\n",
    "                model.fit(self.X_train, self.y_train)\n",
    "                if self.target_type == 'regression':\n",
    "                    self.historical_scores.append(str(r2_score(self.y_test, model.predict(self.X_test))))\n",
    "                else:\n",
    "                    self.historical_scores.append(str(cross_val_score(model, self.data, self.target).mean()))               \n",
    "            else:\n",
    "                fitted_model = model.fit('split_training_data')\n",
    "                self.historical_scores.append(fitted_model.score())\n",
    "                \n",
    "        self.historical_points += next_points['next_points'].split(';')\n",
    "        \n",
    "    def get_best_point(self):\n",
    "        \n",
    "        #if self.target_type == 'regression':\n",
    "        #    best = 1000\n",
    "        #else:\n",
    "        best = -100\n",
    "            \n",
    "        best_point = 'failed'\n",
    "        for s, pt in zip(qc_bo.historical_scores, qc_bo.historical_points):\n",
    "            if self.metric(float(s),  best):\n",
    "                best = float(s)\n",
    "                best_point = pt\n",
    "        return(best_point)\n",
    "    \n",
    "    def create_url (self):\n",
    "\n",
    "#        if self.target_type == 'regression':\n",
    "#            # if lower score is better (rmse, etc)\n",
    "#            h_scores = [float(s) for s in self.historical_scores]\n",
    "#            mx = np.max(h_scores)\n",
    "#            data = json.dumps({'scores': ','.join([str(1+mx-s) for s in h_scores]), 'points': ';'.join(self.historical_points)})\n",
    "#        else:\n",
    "        data = json.dumps({'scores': ','.join([str(s) for s in self.historical_scores]), 'points': ';'.join(self.historical_points)})\n",
    "        \n",
    "        y_best = 10\n",
    "        hp_ranges_str = ';'.join([','.join([str(x) for x in s]) for s in self.hp_ranges])\n",
    "        hp_types_str = ','.join(self.hp_types)\n",
    "        stem = \"http://localhost:8000/bayes_opt?hp_types=\"\n",
    "        url = stem + \"{}&g_batch_size={}&hp_ranges={}&y_best={}&n_gpus={}&use_qc={}\".format(hp_types_str, self.gbr_batch_size, \n",
    "                                                                                               hp_ranges_str, y_best, self.n_procs, 'False')\n",
    "        return url, data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c521f7d-ff87-407f-936d-62ed24a81625",
   "metadata": {},
   "outputs": [],
   "source": [
    "class use_log:\n",
    "    def __init__(self, log_or_not):\n",
    "        self.log_or_not = log_or_not\n",
    "    def log(self, x):\n",
    "        if self.log_or_not:\n",
    "            return np.log(x)\n",
    "        else:\n",
    "            return (x)\n",
    "def show_results(session, log):\n",
    "    u = use_log(log)\n",
    "    h = [u.log(float(q)) for q in session.historical_scores]\n",
    "    print(\"average performance (during bo)               {}\".format(np.mean([float(q) for q in session.historical_scores])))\n",
    "    print(\"standard deviation of performance (during bo) {}\".format(np.std([float(q) for q in session.historical_scores])))\n",
    "#    if session.target_type == 'regression':\n",
    "#        best = 10\n",
    "#    else:\n",
    "    best = -100\n",
    "    \n",
    "    best_so_far = []\n",
    "    for q in session.historical_scores:\n",
    "        if session.metric(u.log(float(q)), best):\n",
    "            best = u.log(float(q))\n",
    "        best_so_far.append(best)\n",
    "    plt.plot(h, label='historical')\n",
    "    plt.plot(best_so_far, label='best_so_far')\n",
    "    p = plt.legend()\n",
    "    print(\"Best after BO {}\".format(best))\n",
    "\n",
    "class bunch:\n",
    "    def __init__(self, d):\n",
    "        self.data = d['data']\n",
    "        self.target = d['target']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6055c145-8d11-4349-b379-eb4b3bea9503",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Elastic Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a85679-0d09-4d88-99f0-a8673f4617ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "qc_bo = bayes_optimization_example(30, 4, 'EN')\n",
    "hp_types = ['float', 'float']\n",
    "hp_ranges =  [[0.0001,.99999],[0.0001,.99999]]\n",
    "qc_bo.initialize(diabetes, hp_types, hp_ranges)\n",
    "historical_qei = []\n",
    "best_points = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c7990d6-d048-40e3-886f-67cc4e2fbc33",
   "metadata": {},
   "outputs": [],
   "source": [
    "for a in range(50):\n",
    "    url, data = qc_bo.create_url() # using historical data\n",
    "    response = requests.post(url, data=data)\n",
    "    next_points = json.loads(response.text)\n",
    "    historical_qei.append(next_points['best_ccdf'])\n",
    "    qc_bo.test_points(next_points)\n",
    "    best_points.append(qc_bo.get_best_point())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd45cb50-616f-441a-8c9a-855cc52dae88",
   "metadata": {},
   "outputs": [],
   "source": [
    "h = [np.log(float(q)) for q in qc_bo.historical_scores]\n",
    "best = 10\n",
    "best_so_far = []\n",
    "for q in qc_bo.historical_scores:\n",
    "    if np.log(float(q))< best:\n",
    "        best = np.log(float(q))\n",
    "    best_so_far.append(best)\n",
    "plt.plot(h)\n",
    "plt.plot(best_so_far)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "082362b0-88b5-4418-899e-b31a534bddd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# when parameters changed\n",
    "plt.plot([float(b.split(',')[0]) for b in best_points], label='alpha')\n",
    "plt.plot([float(b.split(',')[1]) for b in best_points], label='l1_ratio')\n",
    "p = plt.legend()\n",
    "qc_bo.get_best_point()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf89538-3247-4757-9501-75a1a3ff2e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(historical_qei)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a7ea93f-ddd8-42c2-9895-cd87a95c448a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Support Vector Regression\n",
    "C, \n",
    "epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3263a208-2701-4235-9001-91a563c3fa01",
   "metadata": {},
   "outputs": [],
   "source": [
    "qc_bo = bayes_optimization_example(30, 4, 'SVR')\n",
    "hp_types = ['float', 'float']\n",
    "hp_ranges =  [[0.1,10],[0.001,.999]]\n",
    "qc_bo.initialize(diabetes, hp_types, hp_ranges)\n",
    "historical_qei = []\n",
    "best_points = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2821e10e-c1de-4b88-b0a1-79d32acafebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "for a in range(50):\n",
    "    url, data = qc_bo.create_url() # using historical data\n",
    "    response = requests.post(url, data=data)\n",
    "    next_points = json.loads(response.text)\n",
    "    historical_qei.append(next_points['best_ccdf'])\n",
    "    qc_bo.test_points(next_points)\n",
    "    best_points.append(qc_bo.get_best_point())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be3eb3e-a808-4bc8-ae8d-a624f99921b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# when parameters changed\n",
    "plt.plot([float(b.split(',')[0]) for b in best_points], label='C')\n",
    "plt.plot([float(b.split(',')[1]) for b in best_points], label='epsilon')\n",
    "p = plt.legend()\n",
    "qc_bo.get_best_point()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b6ed10-8cda-4ed2-853e-b2740aa5e268",
   "metadata": {},
   "outputs": [],
   "source": [
    "h = [np.log(float(q)) for q in qc_bo.historical_scores]\n",
    "best = 10\n",
    "best_so_far = []\n",
    "for q in qc_bo.historical_scores:\n",
    "    if np.log(float(q))< best:\n",
    "        best = np.log(float(q))\n",
    "    best_so_far.append(best)\n",
    "plt.plot(h)\n",
    "plt.plot(best_so_far)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c37db96-bbfd-4676-b409-93b8e482926c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## XGBoost Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adea2366-b7fe-445b-8ebf-e940b84627ca",
   "metadata": {},
   "source": [
    "### One hot encoding for digits dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba1ebe1-5d91-4393-a350-94659e89b22e",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = OneHotEncoder(handle_unknown='ignore')\n",
    "enc.fit(digits.target.reshape(-1,1))\n",
    "enc.categories_\n",
    "ohe_target = enc.transform(digits.target.reshape(-1, 1))\n",
    "#enc.inverse_transform([[0, 1, 1, 0, 0], [0, 0, 0, 1, 0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67449e5e-ccb2-49f3-b4a2-45488a2412f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "digits_bunch= bunch({'target': ohe_target,\n",
    "                     'data': digits.data})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39981b68-281c-43f1-9ca8-12178e8a7eb1",
   "metadata": {},
   "source": [
    "### Breast Cancer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c312eba9-3e70-46ef-92e0-ce071bbfd7db",
   "metadata": {},
   "outputs": [],
   "source": [
    "qc_bo = bayes_optimization_example(300, 4, 'XGBoost')\n",
    "\n",
    "parameter_names = ['gamma', 'reg_lambda', 'colsample_by_tree',\n",
    "                   'max_depth', 'min_child_weight', 'learning_rate']\n",
    "\n",
    "hp_types = ['float', 'float', 'float',  'int', 'float', 'float']\n",
    "hp_ranges =  [[0.01, .999],[0.001,.999], [0.001,.999],\n",
    "              [2, 5],[0.001,.999] ,[0.001,.999]]\n",
    "\n",
    "qc_bo.initialize(breast_cancer, hp_types, hp_ranges)\n",
    "historical_qei = []\n",
    "best_points = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e1aaab0-12eb-4d57-8201-b89c0c1369b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for a in range(100):\n",
    "    url, data = qc_bo.create_url() # using historical data\n",
    "    start = time.time()\n",
    "    response = requests.post(url, data=data)\n",
    "    print(\"{}: Spent {} seconds getting next points\".format(a, round(time.time()-start,3)))\n",
    "    next_points = json.loads(response.text)\n",
    "    historical_qei.append(next_points['best_ccdf'])\n",
    "    qc_bo.test_points(next_points)\n",
    "    best_points.append(qc_bo.get_best_point())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ef325d-f7d4-4a47-a637-2fe576bc43ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "show_results(qc_bo, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "495d20de-a469-4011-9c3f-f9be862a2cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "[str(p)+': '+z for p, z in zip(parameter_names, qc_bo.get_best_point().split(','))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44adbea8-7dff-4ba2-b982-ef8d4204b768",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## XGBoost Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "665f76c6-9f6b-4838-8f54-1b67016726d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "qc_bo = bayes_optimization_example(300, 4, 'XGBoostRegressor')\n",
    "\n",
    "parameter_names = ['gamma', 'reg_lambda', 'colsample_by_tree',\n",
    "                   'max_depth', 'min_child_weight', 'learning_rate']\n",
    "\n",
    "hp_types = ['float', 'float', 'float',  'int', 'float', 'float']\n",
    "hp_ranges =  [[0.01, .999],[0.001,.999], [0.001,.999],\n",
    "              [2, 5],[0.001,.999] ,[0.001,.999]]\n",
    "\n",
    "qc_bo.initialize(diabetes, hp_types, hp_ranges)\n",
    "historical_qei = []\n",
    "best_points = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf7a269-f200-45a7-8100-807311a254fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "for a in range(10):\n",
    "    url, data = qc_bo.create_url() # using historical data\n",
    "    start = time.time()\n",
    "    response = requests.post(url, data=data)\n",
    "    print(\"{}: Spent {} seconds getting next points\".format(a, round(time.time()-start,3)))\n",
    "    next_points = json.loads(response.text)\n",
    "    historical_qei.append(next_points['best_ccdf'])\n",
    "    qc_bo.test_points(next_points)\n",
    "    best_points.append(qc_bo.get_best_point())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d05fa8e-a57c-4a76-8a4a-02b6ca6e041e",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_results(qc_bo, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f10ad900-90ce-4382-8d27-7513b9062b18",
   "metadata": {},
   "source": [
    "## NVIDIA Hackathon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d322ec0-de18-47c2-8065-cccd2be905f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "qc_bo = bayes_optimization_example(480, 2, 'RandomForestRegressor')\n",
    "# can't tweak max_samples and n_bins together\n",
    "# n_bins can't be too large with large n clusters\n",
    "\n",
    "parameter_names = ['max_depth',  'min_samples_leaf', \n",
    "                   'min_samples_split', 'max_samples', 'min_impurity_decrease'] \n",
    "hp_types = ['int', 'int', 'int', 'int', 'float']\n",
    "hp_ranges =  [[3, 30], [2, 20], [2, 20], [10, 256], [.1,.99]]\n",
    "\n",
    "qc_bo.initialize(breast_cancer, hp_types, hp_ranges)\n",
    "historical_qei = []\n",
    "best_points = []\n",
    "\n",
    "# to do:\n",
    "# flask_worker validate input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad98c2b-66a0-4dbd-8720-8e1934336fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "for a in range(100):\n",
    "    url, data = qc_bo.create_url() # using historical data\n",
    "    start = time.time()\n",
    "    response = requests.post(url, data=data)\n",
    "    print(\"{}: Spent {} seconds getting next points\".format(a, round(time.time()-start,3)))\n",
    "    next_points = json.loads(response.text)\n",
    "    historical_qei.append(next_points['best_ccdf'])\n",
    "    qc_bo.test_points(next_points)\n",
    "    best_points.append(qc_bo.get_best_point())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0029a9b3-4671-4685-9f47-9fa9278dcc4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_results(qc_bo, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "158d4d14-776d-4d71-a273-6a547b88f0c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "[str(p)+': '+z for p, z in zip(parameter_names, get_best_point(qc_bo).split(','))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92484335-5606-459d-885a-400f5fcedd6b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:rapids-venv]",
   "language": "python",
   "name": "conda-env-rapids-venv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
